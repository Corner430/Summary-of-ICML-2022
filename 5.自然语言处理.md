- Pre-trained Language Models
- Self-conditioning Pre-Trained Language Models
- A3T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing
- Improving Language Models by Retrieving from Trillions of Tokens
- UNIREX: A Unified Learning Framework for Language Model Rationale Extraction
- Revisiting Label Smoothing and Knowledge Distillation Compatibility: What was Missing?
- Dialog Inpainting: Turning Documents into Dialogs
- Knowledge Base Question Answering by Case-based Reasoning over Subgraphs
- Weisfeiler-Lehman Meets Gromov-Wasserstein
- Unified Scaling Laws for Routed Language Models
- Context-Aware Drift Detection
- Mitigating Gender Bias in Face Recognition using the von Mises-Fisher Mixture Model
- GLaM: Efficient Scaling of Language Models with Mixture-of-Experts
- A Context-Integrated Transformer-Based Neural Network for Auction Design
- Parametric Visual Program Induction with Function Modularization
- Bayesian Deep Embedding Topic Meta-Learner
- From data to functa: Your data point is a function and you can treat it like one
- Inductive Biases and Variable Creation in Self-Attention Mechanisms
- Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP)
- Principled Knowledge Extrapolation with GANs
- SCHA-VAE: Hierarchical Context Aggregation for Few-Shot Generation
- Understanding and Improving Knowledge Graph Embedding for Entity Alignment
- Dual Perspective of Label-Specific Feature Learning for Multi-Label Classification
- HyperPrompt: Prompt-based Task-Conditioning of Transformers
- Label-Descriptive Patterns and Their Application to Characterizing Classification Errors
- Unsupervised Detection of Contextualized Embedding Bias with Application to Ideology
- Neural Laplace: Learning diverse classes of differential equations in the Laplace domain
- Deep Reference Priors: What is the best way to pretrain a model? 7036-7051
- Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents
- Frustratingly Easy Transferability Estimation
- On the Learning of Non-Autoregressive Transformers
- Deduplicating Training Data Mitigates Privacy Risks in Language Models
- Guided-TTS: A Diffusion Model for Text-to-Speech via Classifier Guidance
- SoQal: Selective Oracle Questioning for Consistency Based Active Learning of Cardiac Signals
- Controlling Conditional Language Models without Catastrophic Forgetting
- Conditional GANs with Auxiliary Discriminative Classifier
- MetAug: Contrastive Learning via Meta Feature Augmentation
- CerDEQ: Certifiable Deep Equilibrium Model
- Let Invariant Rationale Discovery Inspire Graph Contrastive Learning
- HousE: Knowledge Graph Embedding with Householder Parameterization
- Learning Multiscale Transformer Models for Sequence Generation
- StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models
- Generating 3D Molecules for Target Protein Binding
- Asking for Knowledge (AFK): Training RL Agents to Query External Knowledge Using Language
- Generative Cooperative Networks for Natural Language Generation
- Co-training Improves Prompt-based Learning for Large Language Models
- Detecting Biomedical Named Entities in COVID-19 Texts
- Learning to Cut by Looking Ahead: Cutting Plane Selection via Imitation Learning
- Zero-shot AutoML with Pretrained Models
- GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models
- Generative Trees: Adversarial and Copycat
- Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval
- How to Stay Curious while avoiding Noisy TVs using Aleatoric Uncertainty Estimation
- Knowledge-Grounded Self-Rationalization via Extractive and Natural Language Explanations
- In defense of dual-encoders for neural ranking
- Towards Coherent and Consistent Use of Entities in Narrative Generation
- Neural Language Models are not Born Equal to Fit Brain Data, but Training Helps
- ContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers