- Pre-trained Language Models
- Self-conditioning Pre-Trained Language Models
- A3T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing
- Improving Language Models by Retrieving from Trillions of Tokens
- UNIREX: A Unified Learning Framework for Language Model Rationale Extraction
- Revisiting Label Smoothing and Knowledge Distillation Compatibility: What was Missing?
- Dialog Inpainting: Turning Documents into Dialogs
- Knowledge Base Question Answering by Case-based Reasoning over Subgraphs
- Weisfeiler-Lehman Meets Gromov-Wasserstein
- Unified Scaling Laws for Routed Language Models
- Context-Aware Drift Detection
- Mitigating Gender Bias in Face Recognition using the von Mises-Fisher Mixture Model
- GLaM: Efficient Scaling of Language Models with Mixture-of-Experts
- A Context-Integrated Transformer-Based Neural Network for Auction Design
- Parametric Visual Program Induction with Function Modularization
- Bayesian Deep Embedding Topic Meta-Learner
- From data to functa: Your data point is a function and you can treat it like one
- Inductive Biases and Variable Creation in Self-Attention Mechanisms
- Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP)
- Principled Knowledge Extrapolation with GANs
- SCHA-VAE: Hierarchical Context Aggregation for Few-Shot Generation
- Understanding and Improving Knowledge Graph Embedding for Entity Alignment
- Dual Perspective of Label-Specific Feature Learning for Multi-Label Classification
- HyperPrompt: Prompt-based Task-Conditioning of Transformers
- Label-Descriptive Patterns and Their Application to Characterizing Classification Errors
- Unsupervised Detection of Contextualized Embedding Bias with Application to Ideology
- Neural Laplace: Learning diverse classes of differential equations in the Laplace domain
- Deep Reference Priors: What is the best way to pretrain a model? 7036-7051
- Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents
- Frustratingly Easy Transferability Estimation
- On the Learning of Non-Autoregressive Transformers
- Deduplicating Training Data Mitigates Privacy Risks in Language Models
- Guided-TTS: A Diffusion Model for Text-to-Speech via Classifier Guidance
- SoQal: Selective Oracle Questioning for Consistency Based Active Learning of Cardiac Signals
- Controlling Conditional Language Models without Catastrophic Forgetting
- Conditional GANs with Auxiliary Discriminative Classifier
- MetAug: Contrastive Learning via Meta Feature Augmentation
- CerDEQ: Certifiable Deep Equilibrium Model
- Let Invariant Rationale Discovery Inspire Graph Contrastive Learning
- HousE: Knowledge Graph Embedding with Householder Parameterization
- Learning Multiscale Transformer Models for Sequence Generation
- StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models
- Generating 3D Molecules for Target Protein Binding
- Asking for Knowledge (AFK): Training RL Agents to Query External Knowledge Using Language
- Generative Cooperative Networks for Natural Language Generation
- Co-training Improves Prompt-based Learning for Large Language Models
- Detecting Biomedical Named Entities in COVID-19 Texts
- Learning to Cut by Looking Ahead: Cutting Plane Selection via Imitation Learning
- Zero-shot AutoML with Pretrained Models
- GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models
- Generative Trees: Adversarial and Copycat
- Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval
- How to Stay Curious while avoiding Noisy TVs using Aleatoric Uncertainty Estimation
- Knowledge-Grounded Self-Rationalization via Extractive and Natural Language Explanations
- In defense of dual-encoders for neural ranking
- Towards Coherent and Consistent Use of Entities in Narrative Generation
- Neural Language Models are not Born Equal to Fit Brain Data, but Training Helps
- ContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers
- Constrained Optimization with Dynamic Bound-scaling for Effective NLP Backdoor Defense
- Staged Training for Transformer Language Models
- Bit Prioritization in Variational Autoencoders via Progressive Coding
- The Multivariate Community Hawkes Model for Dependent Relational Events in Continuous-time Networks
- TAM: Topology-Aware Margin Loss for Class-Imbalanced Node Classification
- Black-Box Tuning for Language-Model-as-a-Service
- Cross-Space Active Learning on Graph Convolutional Networks
- Fast Lossless Neural Compression with Integer-Only Discrete Flows
- Certifying Out-of-Domain Generalization for Blackbox Functions
- To Smooth or Not? When Label Smoothing Meets Noisy Labels
- BabelTower: Learning to Auto-parallelized Program Translation
- Omni-Granular Ego-Semantic Propagation for Self-Supervised Graph Representation Learning
- Fourier Learning with Cyclical Data
- Identity-Disentangled Adversarial Augmentation for Self-supervised Learning
- Learning from a Learning User for Optimal Recommendations
- NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework
- De novo mass spectrometry peptide sequencing with a transformer model
- Pure Noise to the Rescue of Insufficient Data: Improving Imbalanced Classification by Training on Random Noise Images
- Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts
- Position Prediction as an Effective Pretraining Strategy
- Anytime Information Cascade Popularity Prediction via Self-Exciting Processes
- Examining Scaling and Transfer of Language Model Architectures for Machine Translation
- Revisiting End-to-End Speech-to-Text Translation From Scratch
- Prototype-Anchored Learning for Learning with Imperfect Annotations
- Neural-Symbolic Models for Logical Queries on Knowledge Graphs