- How to Train Your Wide Neural Network Without Backprop: An Input-Weight Alignment Perspective
- DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks
- Wide Bayesian neural networks have a simple weight posterior: theory and accelerated sampling
- Learning to Hash Robustly, Guaranteed
- Coarsening the Granularity: Towards Structurally Sparse Lottery Tickets
- Data-Efficient Double-Win Lottery Tickets from Robust Pre-training
- Monarch: Expressive Structured Matrices for Efficient and Accurate Training
- Finding the Task-Optimal Low-Bit Sub-Distribution in Deep Neural Networks
- Adapting to Mixing Time in Stochastic Optimization with Markovian Data
- Revisiting the Effects of Stochasticity for Hamiltonian Samplers
- Fast and Reliable Evaluation of Adversarial Robustness with Minimum-Margin Attack
- Random Gegenbauer Features for Scalable Kernel Methods
- Sparse Double Descent: Where Network Pruning Aggravates Overfitting
- Forget-free Continual Learning with Winning Subnetworks
- Robustness Implies Generalization via Data-Dependent Generalization Bounds
- Generating Distributional Adversarial Examples to Evade Statistical Detectors
- Secure Quantized Training for Deep Learning
- Soft Truncation: A Universal Training Technique of Score-based Diffusion Model for High Precision Score Estimation
- Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters
- Robust Fine-Tuning of Deep Neural Networks with Hessian-based Generalization Guarantees
- SDQ: Stochastic Differentiable Quantization with Mixed Precision
- Double Sampling Randomized Smoothing
- Decentralized Online Convex Optimization in Networked Systems
- Feature Learning and Signal Propagation in Deep Neural Networks
- Channel Importance Matters in Few-Shot Image Classification
- Optimizing Tensor Network Contraction Using Reinforcement Learning
- Bounding the Width of Neural Networks via Coupled Initialization A Worst Case Analysis
- Constants Matter: The Performance Gains of Active Learning
- AutoSNN: Towards Energy-Efficient Spiking Neural Networks
- Implicit Bias of the Step Size in Linear Diagonal Neural Networks
- DNNR: Differential Nearest Neighbors Regression
- Overcoming Oscillations in Quantization-Aware Training
- Efficient Test-Time Model Adaptation without Forgetting
- Fast Finite Width Neural Tangent Kernel
- Multicoated Supermasks Enhance Hidden Networks
- POET: Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging
- Large-scale Stochastic Optimization of NDCG Surrogates for Deep Learning with Provable Convergence
- Winning the Lottery Ahead of Time: Efficient Early Network Pruning
- Optimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training