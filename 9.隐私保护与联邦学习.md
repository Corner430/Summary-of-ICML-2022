- Public Data-Assisted Mirror Descent for Private Model Training
- The Poisson Binomial Mechanism for Unbiased Federated Learning with Secure Aggregation
- Private optimization in the interpolation regime: faster rates and hardness results
- Approximate Bayesian Computation with Domain Expert in the Loop
- VariGrow: Variational Architecture Growing for Task-Agnostic Continual Learning based on Bayesian Novelty
- Stability Based Generalization Bounds for Exponential Family Langevin Dynamics
- Estimating the Optimal Covariance with Imperfect Mean in Diffusion Probabilistic Models
- Sparse Mixed Linear Regression with Guarantees: Taming an Intractable Problem with Invex Relaxation
- Scalable Spike-and-Slab
- H-Consistency Bounds for Surrogate Loss Minimizers
- Nearly Optimal Catoni's M-estimator for Infinite Variance
- Compressed-VFL: Communication-Efficient Learning with Vertically Partitioned Data
- The Fundamental Price of Secure Aggregation in Differentially Private Federated Learning
- Selective Network Linearization for Efficient Private Inference
- Selling Data To a Machine Learner: Pricing via Costly Signaling
- Task-aware Privacy Preservation for Multi-dimensional Data
- DisPFL: Towards Communication-Efficient Personalized Federated Learning via Decentralized Sparse Training
- Privacy for Free: How does Dataset Condensation Help Privacy? 5378-5396
- FedNew: A Communication-Efficient and Privacy-Preserving Newton-Type Method for Federated Learning
- An Equivalence Between Data Poisoning and Byzantine Gradient Attacks
- Byzantine Machine Learning Made Easy By Resilient Averaging of Momentums
- Private frequency estimation via projective geometry
- Faster Privacy Accounting via Evolving Discretization
- A Joint Exponential Mechanism For Differentially Private Top-k
- Secure Distributed Training at Scale
- Bounding Training Data Reconstruction in Private (Deep) Learning
- Private Streaming SCO in â„“p geometry with Applications in High Dimensional Online Decision Making
- Deduplicating Training Data Mitigates Privacy Risks in Language Models
- Differentially Private Approximate Quantiles
- Dataset Condensation via Efficient Synthetic-Data Parameterization
- Differentially Private Maximal Information Coefficients
- Low-Complexity Deep Convolutional Neural Networks on Fully Homomorphic Encryption Using Multiplexed Parallel Convolutions
- Query-Efficient and Scalable Black-Box Adversarial Attacks on Discrete Sequential Data via Bayesian Optimization
- Tight and Robust Private Mean Estimation with Few Users
- Improved Regret for Differentially Private Exploration in Linear MDP
- Virtual Homogeneity Learning: Defending against Data Heterogeneity in Federated Learning
- Anarchic Federated Learning
- QSFL: A Two-Level Uplink Communication Optimization Framework for Federated Learning
- Bitwidth Heterogeneous Federated Learning with Progressive Weight Dequantization
- Personalized Federated Learning via Variational Bayesian Inference
- Federated Learning with Label Distribution Skew via Logits Calibration
- Neurotoxin: Durable Backdoors in Federated Learning
- Topology-aware Generalization of Decentralized SGD
- Resilient and Communication Efficient Learning for Heterogeneous Federated Systems