- Adapting the Linearised Laplace Model Evidence for Modern Deep Learning
- Personalization Improves Privacy-Accuracy Tradeoffs in Federated Learning
- IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages
- YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for Everyone
- Fairness with Adaptive Weights
- DisPFL: Towards Communication-Efficient Personalized Federated Learning via Decentralized Sparse Training
- Transfer and Marginalize: Explaining Away Label Noise with Privileged Information
- Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning
- Matching Structure for Dual Learning
- IDYNO: Learning Nonparametric DAGs from Interventional Dynamic Data
- Identifiability Conditions for Domain Adaptation
- The Role of Deconfounding in Meta-learning
- Subspace Learning for Effective Meta-Learning
- Domain Adaptation for Time Series Forecasting via Attention Sharing
- Robust alignment of cross-session recordings of neural population activity by behaviour via unsupervised domain adaptation
- Comprehensive Analysis of Negative Sampling in Knowledge Graph Representation Learning
- Matching Learned Causal Effects of Neural Networks with Domain Priors
- Deduplicating Training Data Mitigates Privacy Risks in Language Models
- Lyapunov Density Models: Constraining Distribution Shift in Learning-Based Control
- Learning fair representation with a parametric integral probability metric
- Transfer Learning In Differential Privacy's Hybrid-Model
- Partial disentanglement for domain adaptation
- Balancing Discriminability and Transferability for Source-Free Domain Adaptation
- DAdaQuant: Doubly-adaptive quantization for communication-efficient Federated Learning
- Plan Your Target and Learn Your Skills: Transferable State-Only Imitation Learning via Decoupled Policy Optimization
- Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering
- Equivariant Priors for compressed sensing with unknown orientation
- Architecture Agnostic Federated Learning for Neural Networks
- CtrlFormer: Learning Transferable State Representation for Visual Control via Transformer
- PAC-Net: A Model Pruning Approach to Inductive Transfer Learning
- On Transportation of Mini-batches: A Hierarchical Approach
- Improving Mini-batch Optimal Transport via Partial Transportation
- The Unsurprising Effectiveness of Pre-Trained Vision Models for Control
- A Closer Look at Smoothness in Domain Adversarial Training