- Certified Neural Network Watermarks with Randomized Smoothing
- Inducing Causal Structure for Interpretable Neural Networks
- Meaningfully debugging model mistakes using conceptual counterfactual explanations
- XAI for Transformers: Better Explanations through Conservative Propagation
- Interactive Correlation Clustering with Existential Cluster Constraints
- State Transition of Dendritic Spines Improves Learning of Sparse Spiking Neural Networks
- Neural Inverse Kinematic
- Set Based Stochastic Subsampling
- On the Surrogate Gap between Contrastive and Supervised Losses
- A query-optimal algorithm for finding counterfactuals
- Deploying Convolutional Networks on Untrusted Platforms Using 2D Holographic Reduced Representations
- Towards Understanding Sharpness-Aware Minimization
- Causal structure-based root cause analysis of outliers
- Linearity Grafting: Relaxed Neuron Pruning Helps Certifiable Robustness
- Counterfactual Transportability: A Formal Approach
- Framework for Evaluating Faithfulness of Local Explanations
- Fair Generalized Linear Models with a Convex Penalty
- Learning Iterative Reasoning through Energy Minimization
- Model-Value Inconsistency as a Signal for Epistemic Uncertainty
- A Functional Information Perspective on Model Interpretation
- Inducing Causal Structure for Interpretable Neural Networks
- Plug-In Inversion: Model-Agnostic Inversion for Vision with Data Augmentations
- Contextual Information-Directed Sampling
- GSmooth: Certified Robustness against Semantic Transformations via Generalized Randomized Smoothing
- Lazy Estimation of Variable Importance for Large Neural Networks
- SPDY: Accurate Pruning with Speedup Guarantees
- Efficient Approximate Inference for Stationary Kernel on Frequency Domain
- Neural Network Poisson Models for Behavioural and Neural Spike Train Data
- Sanity Simulations for Saliency Methods
- Adaptive Data Analysis with Correlated Observations
- Datamodels: Understanding Predictions with Data and Data with Predictions
- Implicit Bias of Linear Equivariant Networks
- Rethinking Attention-Model Explainability through Faithfulness Violation Test
- A Rigorous Study of Integrated Gradients Method and Extensions to Internal Neuron Attributions
- Interpretable Neural Networks with Frank-Wolfe: Sparse Relevance Maps and Relevance Orderings
- Tell me why! Explanations support learning relational and causal structure
- Robustness in Multi-Objective Submodular Optimization: a Quantile Approach
- Wide Neural Networks Forget Less Catastrophically
- Memory-Based Model Editing at Scale
- Invariant Ancestry Search
- SpeqNets: Sparsity-aware permutation-equivariant graph networks
- Measuring Representational Robustness of Neural Networks Through Shared Invariances
- Stable Conformal Prediction Sets
- Discovering Generalizable Spatial Goal Representations via Graph-based Active Reward Learning
- A Study on the Ramanujan Graph Property of Winning Lottery Tickets
- Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness
- Debiaser Beware: Pitfalls of Centering Regularized Transport Maps
- Adaptive Second Order Coresets for Data-efficient Machine Learning
- On the Practicality of Deterministic Epistemic Uncertainty
- A Simple Guard for Learned Optimizers
- Hardness and Algorithms for Robust and Sparse Optimization
- Nonlinear Feature Diffusion on Hypergraphs
- Universal Joint Approximation of Manifolds and Densities by Simple Injective Flows
- The Teaching Dimension of Regularized Kernel Learners
- Spectral Representation of Robustness Measures for Optimization Under Input Uncertainty
- A Consistent and Efficient Evaluation Strategy for Attribution Methods
- Direct Behavior Specification via Constrained Reinforcement Learning
- Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers
- Accelerating Shapley Explanation via Contributive Cooperator Selection
- Robust Models Are More Interpretable Because Attributions Look Normal
- Fishing for User Data in Large-Batch Federated Learning via Gradient Magnification
- Synergy and Symmetry in Deep Learning: Interactions between the Data, Model, and Inference Algorithm
- Analyzing and Mitigating Interference in Neural Architecture Search
- A Theoretical Analysis on Independence-driven Importance Weighting for Covariate-shift Generalization
- Investigating Why Contrastive Learning Benefits Robustness against Label Noise
- Active fairness auditing
- Injecting Logical Constraints into Neural Networks via Straight-Through Estimators
- Set Norm and Equivariant Skip Connections: Putting the Deep in Deep Sets
- Learning to Solve PDE-constrained Inverse Problems with Graph Networks